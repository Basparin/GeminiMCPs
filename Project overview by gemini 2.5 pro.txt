  Project: CodeSage MCP Server

  Project Overview
  The CodeSage Model Context Protocol (MCP) Server is designed to extend the capabilities of the Gemini CLI,
  particularly in the areas of code analysis and context management. It functions as an intermediary, enabling the
  Gemini CLI to interact with larger codebases and integrate with various Large Language Models (LLMs) for
  specialized tasks.

  Key Technologies:
   * Python: The primary programming language.
   * FastAPI: Used for building the web API that handles JSON-RPC requests.
   * Uvicorn: An ASGI server that runs the FastAPI application.
   * python-dotenv: For managing environment variables, especially for API keys.
   * groq: The Python client for the Groq LLM API.
   * openai: The official OpenAI Python client, used to interact with the OpenRouter API.
   * google-generativeai: The Python client for Google AI models.
   * pytest: For running the test suite.
   * Docker: For containerization and easy deployment.

  Architecture:
  The server is a FastAPI application that exposes a JSON-RPC endpoint (/mcp). This endpoint handles requests for
  tool discovery (initialize, tools/list) and tool execution (tools/call). It integrates with a CodebaseManager for
  file system operations, codebase indexing, and LLM integration.

  Building and Running

  Setup
   1. Clone the repository:
   1     git clone <repository_url>
   2     cd GeminiMCPs
   2. Install dependencies into the virtual environment:
   1     venv/bin/pip install -r requirements.txt

  Running the Server
  You can run the CodeSage MCP server either directly using uvicorn or via Docker Compose.

  Running Directly (using uvicorn)
  To start the CodeSage MCP server:
   1. Navigate to the project root directory (/home/basparin/Escritorio/GeminiMCPs).
   2. Execute the following command:
   1     uvicorn codesage_mcp.main:app --host 127.0.0.1 --port 8000
      The server will then be accessible at http://127.0.0.1:8000.

  Running with Docker Compose
  Ensure you have Docker and Docker Compose installed. From the project root directory, run:

   1 docker compose up --build

  This will build the Docker image (if not already built) and start the server in a container. The server will be
  accessible at http://localhost:8000.

  Configuring Gemini CLI
  To enable the Gemini CLI to utilize the CodeSage MCP server, you need to add its configuration to your Gemini CLI
  settings.json file. This file is typically located at ~/.config/gemini-cli/settings.json on Linux/macOS or
  %APPDATA%\gemini-cli\settings.json on Windows.

  Add the following entry to the mcpServers array in your settings.json:

   1 {
   2   "mcpServers": [
   3     {
   4       "name": "codesage",
   5       "httpUrl": "http://127.0.0.1:8000",
   6       "trust": true
   7     }
   8   ]
   9 }
  After adding this configuration, restart your Gemini CLI session for the changes to take effect. You should then
  be able to discover and use the tools exposed by the CodeSage MCP server.

  Development Conventions

  Tooling:
  The project leverages fastapi for API development, uvicorn for serving the application, python-dotenv for secure
  management of environment variables, groq, openai, and google-generativeai for LLM integration, pytest for
  testing, and ruff for linting and formatting.

  Code Structure:
   * `codesage_mcp/main.py`: Contains the main FastAPI application logic, handling JSON-RPC requests and routing them
     to the appropriate tool functions.
   * `codesage_mcp/tools.py`: Defines the individual tool functions that the MCP server exposes to the Gemini CLI.
   * `codesage_mcp/codebase_manager.py`: Manages file system operations, codebase indexing, and LLM integration.
   * `codesage_mcp/config.py`: Loads API keys for various LLMs from environment variables.
   * `tests/`: Contains the test suite for the project, with tests for the CodebaseManager and the FastAPI
     application.

  Current Tools Implemented:
   * read_code_file: Reads and returns the content of a specified code file.
   * index_codebase: Indexes a given codebase path for analysis. The index is persistent and respects .gitignore.
   * search_codebase: Searches for a pattern within indexed code files.
   * get_file_structure: Provides a high-level overview of a file's structure.
   * summarize_code_section: Summarizes a specific section of code using the Groq, OpenRouter, or Google AI APIs.
   * semantic_search_codebase: Performs a semantic search within the indexed codebase to find code snippets
     semantically similar to the given query.
   * find_duplicate_code: Identifies duplicate or highly similar code sections within the indexed codebase using
     semantic similarity analysis.
   * list_undocumented_functions: Identifies and lists Python functions in a specified file that are missing
     docstrings.
   * count_lines_of_code: Counts lines of code (LOC) in the indexed codebase, providing a summary by file type.
   * get_dependencies_overview: Analyzes Python files in the indexed codebase and extracts import statements,
     providing a high-level overview of internal and external dependencies.
   * configure_api_key: Configures API keys for LLMs (e.g., Groq, OpenRouter, Google AI).

  Planned Features:
  The project aims to implement more advanced features, including:
   * More advanced search capabilities (e.g., semantic search).


  CodeSage MCP Server: Workspace Context Summary

  This document provides a comprehensive analysis of the CodeSage MCP Server workspace. It is intended to serve as a
  detailed summary for other AI agents to formulate a strategic roadmap for future development.

  ---

  1. Project Overview

   * Main Purpose: The CodeSage MCP Server is a Python-based Model Context Protocol (MCP) server designed to
     significantly enhance the capabilities of the Gemini CLI. It acts as a sophisticated intermediary, allowing the
     CLI to interact with large, complex codebases and leverage multiple Large Language Models (LLMs) for advanced
     code analysis, summarization, and generation tasks.

   * Key Technologies:
       * Backend: Python, FastAPI (for the JSON-RPC API), Uvicorn (as the ASGI server).
       * LLM Integration: Clients for Groq (groq), OpenRouter (openai), and Google AI (google-generativeai).
       * Code Analysis: sentence-transformers and faiss-cpu for semantic search and duplicate code detection, radon
         for cyclomatic complexity, and Python's built-in ast module for structural analysis.
       * Development & QA: pytest for testing, ruff for linting and formatting, and pre-commit for automated quality
         checks.
       * Configuration: python-dotenv for managing environment variables and API keys.
       * Deployment: Docker and docker-compose for containerization.

   * High-Level Architecture: The server is a modular FastAPI application. All interactions are handled through a
     single /mcp JSON-RPC endpoint, which routes requests to the appropriate tools. The core logic is orchestrated by
     the CodebaseManager, which was recently refactored to delegate responsibilities to specialized managers for
     indexing (IndexingManager), searching (SearchingManager), and LLM-based analysis (LLMAnalysisManager). This
     architecture promotes separation of concerns and scalability.

  ---

  2. Codebase Structure and Health

   * Directory Structure:
       * codesage_mcp/: Contains the primary application source code, including the FastAPI main.py, tools.py
         definitions, and the core logic modules (codebase_manager.py, indexing.py, searching.py, llm_analysis.py).
       * tests/: Holds the project's test suite, utilizing pytest.
       * docs/: Contains project documentation, notably the tools_reference.md.
       * scripts/: Includes utility scripts for development tasks, such as generate_tools_reference.py.

   * Code Quality and Health: The codebase demonstrates a strong commitment to quality.
       * Linting: The project uses ruff for linting and formatting, enforced via a .pre-commit-config.yaml. The
         AUTONOMOUS_ACTION_REGISTRY.md file notes that an agent recently fixed 146 Ruff errors, bringing the current
         error count to 0.
       * Modularity: A recent, significant refactoring of CodebaseManager has improved modularity by delegating tasks
         to more specialized managers. This is a positive indicator of ongoing architectural improvement.
       * Documentation & Issues: Analysis from the AUTONOMOUS_ACTION_REGISTRY.md shows 0 undocumented functions in key
         files and 0 active TODO/FIXME comments in the codebase, suggesting good maintenance hygiene.
       * Duplicate Code: The find_duplicate_code tool has identified some duplication in documentation files
         (AGENT_WORKFLOW.md vs. an enhanced version, GEMINI.md vs. a lowercase version) and an archived script, which
         are minor issues.

  ---

  3. Tools Implemented

  The MCP server exposes a rich and expanding set of tools. The main.py file lists 21 distinct tools available via
  the JSON-RPC interface.

   * Core Tools:
       * read_code_file: Reads the content of a file.
       * index_codebase: Indexes a codebase for analysis, respecting .gitignore.
       * search_codebase: Performs regex-based search in indexed files.
       * get_file_structure: Provides a structural overview (classes, functions) of a file using AST.

   * LLM-Powered & Semantic Tools:
       * summarize_code_section: Summarizes code using a chosen LLM (Groq, OpenRouter, Google AI).
       * semantic_search_codebase: Finds code snippets semantically similar to a query.
       * find_duplicate_code: Detects duplicate/similar code blocks using semantic analysis.
       * suggest_code_improvements: Analyzes code and suggests improvements using LLMs.
       * generate_unit_tests: Generates unit tests for Python functions.
       * auto_document_tool: Automatically generates documentation for tools.
       * resolve_todo_fixme: Suggests resolutions for TODO/FIXME comments.
       * parse_llm_response: Extracts and validates JSON from raw LLM output.
       * generate_llm_api_wrapper: Generates Python wrapper code for LLM APIs.

   * Code Quality & Analysis Tools:
       * list_undocumented_functions: Finds Python functions missing docstrings.
       * count_lines_of_code: Counts LOC and provides a summary by file type.
       * get_dependencies_overview: Analyzes import statements to map internal and external dependencies.
       * profile_code_performance: Profiles function performance using cProfile.
       * analyze_codebase_improvements: Provides a high-level analysis of potential improvements.

   * Configuration:
       * configure_api_key: Sets API keys for LLM providers.
       * get_configuration: Returns the current configuration with masked API keys.

  ---

  4. Development Workflow and Agent Interaction

   * Core Workflow (`AGENT_WORKFLOW.md`): The established autonomous workflow is an iterative loop: Understand ->
     Plan -> Implement -> Verify -> Communicate. This process emphasizes deep analysis before action, planning that
     anticipates dependencies and errors, and self-verification using the project's established linting and testing
     procedures. It also includes a self-improvement cycle where agents can develop new tools to enhance their own
     capabilities.

   * Agent Interaction Model: The workspace is designed for parallel work by multiple AI agents (Gemini and Qwen). To
     prevent conflicts, each agent has its own designated "scratchpad" file:
       * ACTION_REGISTRY.md: Used by the Gemini agent.
       * AUTONOMOUS_ACTION_REGISTRY.md: Used by the Qwen agent.
       * This separation allows agents to log their actions and analyses independently. Direct inter-agent
         communication is discouraged; coordination is expected to happen through the user.

   * Observed Agent Behavior: The ACTION_REGISTRY.md contains an analysis by the Gemini agent of the Qwen agent's
     work. It describes Qwen as a highly capable but non-transparent "ghost writer" that performs significant work
     (e.g., implementing new tools, running tests) but does not commit its changes to the repository, making
     collaboration and review difficult.

  ---

  5. Identified Opportunities and Challenges

   * Challenges:
       * Agent Transparency: The most significant challenge is the "ghost writer" behavior of the Qwen agent. Its
         failure to commit work undermines collaboration, version control, and project transparency.
       * Context Management: The very existence of this project highlights the ongoing challenge of providing LLMs
         with sufficient context to perform complex software engineering tasks effectively. The tools are all aimed
         at mitigating this core problem.

   * 10 Inferred Tool Development Ideas: Based on the project's trajectory and explicit goals mentioned in the
     documentation, the following 10 tools would be logical next steps:
       1. Security Audit Tool: Explicitly noted as the next short-term goal. This would scan for common security
          vulnerabilities in the codebase.
       2. Performance Optimization Tool: A mid-term goal to build upon the existing profiling tool, perhaps by
          automatically identifying and suggesting optimizations for bottlenecks.
       3. Codebase Visualization Tool: A long-term goal to create visual representations of dependency graphs, code
          complexity, or test coverage.
       4. Automated Refactoring Tool: A natural evolution of suggest_code_improvements that would not just suggest but
          also safely apply common refactorings.
       5. Test Coverage Analysis Tool: To complement generate_unit_tests, this tool would analyze the test suite's
          effectiveness by measuring code coverage.
       6. Dependency Vulnerability Scanner: An extension of get_dependencies_overview that would check the project's
          third-party dependencies against known vulnerability databases (e.g., CVEs).
       7. Git History Analysis Tool: A tool to analyze commit history to extract insights about code churn, developer
          contribution patterns, and the impact of changes over time. This could also help audit agent contributions.
       8. Architectural Smell Detector: A more advanced analysis tool to detect architectural anti-patterns or "code
          smells" that ruff might not catch, such as overly complex object relationships or poor module cohesion.
       9. Automated Bug Finder/Fixer: An ambitious tool that would use static analysis and LLMs to proactively identify
          potential bugs (beyond simple linting errors) and propose or even implement fixes.
       10. Project Scaffolding Tool: A generator tool to create boilerplate for new modules, tools, or tests that
           automatically conform to the project's established conventions and style.
