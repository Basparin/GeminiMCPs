name: Performance Benchmarking

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      server_url:
        description: 'MCP Server URL'
        required: false
        default: 'http://localhost:8000/mcp'
      test_iterations:
        description: 'Number of test iterations'
        required: false
        default: '5'
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '10'
      enable_edge_cases:
        description: 'Enable edge case testing'
        required: false
        default: 'true'

  # Scheduled runs (daily at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'

  # Push to main branch (optional, can be disabled for performance)
  push:
    branches: [ main ]
    paths:
      - 'codesage_mcp/**'
      - 'tests/benchmark_*.py'
      - 'requirements.txt'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      # Start MCP server in Docker
      mcp-server:
        image: codesage-mcp:latest
        ports:
          - 8000:8000
        options: >-
          --health-cmd "curl -f http://localhost:8000/ || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark requests

    - name: Wait for MCP server to be ready
      run: |
        echo "Waiting for MCP server to be healthy..."
        for i in {1..30}; do
          if curl -f http://localhost:8000/ >/dev/null 2>&1; then
            echo "MCP server is ready!"
            break
          fi
          echo "Waiting... ($i/30)"
          sleep 10
        done

        if [ $i -eq 30 ]; then
          echo "MCP server failed to start"
          exit 1
        fi

    - name: Run comprehensive performance benchmarks
      env:
        SERVER_URL: ${{ github.event.inputs.server_url || 'http://localhost:8000/mcp' }}
        TEST_ITERATIONS: ${{ github.event.inputs.test_iterations || '5' }}
        CONCURRENT_USERS: ${{ github.event.inputs.concurrent_users || '10' }}
        ENABLE_EDGE_CASES: ${{ github.event.inputs.enable_edge_cases || 'true' }}
      run: |
        echo "Running comprehensive performance benchmarks..."
        echo "Server URL: $SERVER_URL"
        echo "Test iterations: $TEST_ITERATIONS"
        echo "Concurrent users: $CONCURRENT_USERS"
        echo "Edge cases enabled: $ENABLE_EDGE_CASES"

        # Run main benchmark suite
        python tests/benchmark_performance.py $SERVER_URL

        # Run modular tool benchmarks
        python tests/benchmark_mcp_tools.py $SERVER_URL $TEST_ITERATIONS

    - name: Generate benchmark report
      run: |
        echo "Generating benchmark report..."
        python scripts/generate_benchmark_report.py

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ github.run_number }}
        path: |
          benchmark_results/
          test_results_report.md
        retention-days: 30

    - name: Compare with baseline
      run: |
        echo "Comparing results with baseline..."
        python scripts/compare_benchmark_results.py benchmark_results/ baseline_results.json

    - name: Send notification on failure
      if: failure()
      run: |
        echo "Benchmark failed! Check the logs for details."
        # Add notification logic here (Slack, email, etc.)

  parameterized-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        include:
          # Different server configurations
          - server_config: "default"
            concurrent_users: 5
            test_iterations: 3
          - server_config: "high_load"
            concurrent_users: 20
            test_iterations: 5
          - server_config: "stress_test"
            concurrent_users: 50
            test_iterations: 10

          # Different codebase sizes
          - codebase_size: "small"
            concurrent_users: 10
            test_iterations: 5
          - codebase_size: "medium"
            concurrent_users: 10
            test_iterations: 5
          - codebase_size: "large"
            concurrent_users: 5
            test_iterations: 3

    services:
      mcp-server:
        image: codesage-mcp:latest
        ports:
          - 8000:8000
        env:
          CODEBASE_SIZE: ${{ matrix.codebase_size || 'medium' }}
        options: >-
          --health-cmd "curl -f http://localhost:8000/mcp || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest requests

    - name: Wait for MCP server
      run: |
        for i in {1..30}; do
          if curl -f http://localhost:8000/mcp >/dev/null 2>&1; then
            break
          fi
          sleep 5
        done

    - name: Run parameterized benchmarks
      env:
        CONFIG_NAME: ${{ matrix.server_config || matrix.codebase_size || 'default' }}
        CONCURRENT_USERS: ${{ matrix.concurrent_users }}
        TEST_ITERATIONS: ${{ matrix.test_iterations }}
      run: |
        echo "Running parameterized benchmark: $CONFIG_NAME"
        echo "Concurrent users: $CONCURRENT_USERS"
        echo "Test iterations: $TEST_ITERATIONS"

        # Run with specific parameters
        python scripts/run_parameterized_benchmark.py \
          --config $CONFIG_NAME \
          --users $CONCURRENT_USERS \
          --iterations $TEST_ITERATIONS

    - name: Upload parameterized results
      uses: actions/upload-artifact@v3
      with:
        name: parameterized-results-${{ matrix.server_config }}-${{ matrix.codebase_size }}-${{ github.run_number }}
        path: benchmark_results/
        retention-days: 30

  benchmark-analysis:
    runs-on: ubuntu-latest
    needs: [benchmark, parameterized-benchmark]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download benchmark artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/

    - name: Analyze benchmark trends
      run: |
        echo "Analyzing benchmark trends..."
        python scripts/analyze_benchmark_trends.py artifacts/

    - name: Generate performance insights
      run: |
        echo "Generating performance insights..."
        python scripts/generate_performance_insights.py artifacts/

    - name: Update baseline if improved
      run: |
        echo "Checking if baseline should be updated..."
        python scripts/update_baseline.py artifacts/

    - name: Create GitHub issue on regression
      if: failure()
      run: |
        echo "Creating GitHub issue for performance regression..."
        # Add logic to create GitHub issue

    - name: Publish benchmark dashboard
      run: |
        echo "Publishing benchmark dashboard..."
        # Add logic to publish dashboard (GitHub Pages, etc.)